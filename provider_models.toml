# Suggested models for different providers.
# This file is typically located in ~/.config/mcp/provider_models.toml
#
# The 'model' command will list these for the active provider when called without arguments.
# You can still use 'model <custom_model_name>' even if it's not listed here.

[openai]
models = [
    "o3-mini",
    "gpt-4o",
    "o1",
    "gpt-4o-mini", # Kept correct name
    "gpt-4.5",
    "gpt-4-turbo",
    "gpt-3.5-turbo",
]

[anthropic]
models = [
    "claude-3-7-sonnet-20250219", # Latest Sonnet
    "claude-3-5-sonnet-20241022", # New 3.5 Sonnet
    "claude-3-5-haiku-20241022",  # New 3.5 Haiku
    "claude-3-opus-20240229",     # Opus
    "claude-3-sonnet-20240229",   # Original 3 Sonnet
    "claude-3-haiku-20240307",    # Original 3 Haiku
    "claude-2.1",
    "claude-2.0",
    # "claude-3-5-sonnet-20240620", # Old 3.5 Sonnet (commented out)
]

[gemini]
models = [
    "gemini-2.5-pro-preview-03-25",
    "gemini-2.5-flash",
    "gemini-2.0-flash-lite",
    "gemini-1.5-pro-002",
    "gemini-1.5-flash-002",
]

[ollama]
models = [
    "llama3",
    "llama3:70b",
    "mistral",
    "codellama",
    "llava",
    "gemma:7b",
    "phi3",
    "deepseek-coder",
    "qwen2",
]

[deepseek]
models = ["deepseek-chat", "deepseek-reasoner"]

[groq]
models = [
    "llama3-8b-8192",
    "llama3-70b-8192",
    "llama-3.1-8b-instant",
    "llama-3.2-1b-preview",
    "llama-3.2-3b-preview",
    "llama-3.2-11b-vision-preview",
    "llama-3.2-90b-vision-preview",
    "llama-3.3-70b-specdec",
    "llama-3.3-70b-versatile",
    "llama-3.3-70b",
    "llama-guard-3-8b",
    "mixtral-8x7b-32768",
    "gemma-7b-it",
    "gemma2-9b-it",
    "mistral-saba-24b",
    "qwen-2.5-32b",
    "qwen-2.5-coder-32b",
    "qwen-qwq-32b",
]

[openrouter]
models = [
  "openai/gpt-4o",
  "anthropic/claude-3-opus-20240229",
  "anthropic/claude-3-5-sonnet-20240620",
  "meta-llama/llama-3-70b-instruct",
  "mistral/mistral-large-latest",
  "google/gemini-pro"
]

# [openrouter] # This section is being replaced, remove the old one below
# models = [
#     "openrouter/optimus-alpha",
    # OpenAI
    "openai/gpt-4o",
    "openai/gpt-4o-mini",
    # Anthropic
    "anthropic/claude-3-opus",
    "anthropic/claude-3.5-sonnet",
    "anthropic/claude-3.7-sonnet",
    "anthropic/claude-3.7-sonnet:thinking",
    # Google
    "google/gemini-2.5-pro",
    "google/gemini-2.5-pro-exp-03-25:free",
    "google/gemini-1.5-flash",
    "google/gemini-flash-1.5-8b",
    "google/gemini-flash-2.0",
    "google/gemini-2.0-flash-001",
    "google/gemini-flash-2.0-lite",
    # Mistral
    "mistralai/mistral-large",
    "mistralai/mistral-nemo",
    # Meta
    "meta-llama/llama-3-70b-instruct",
    "meta-llama/llama-3.1-8b-instruct",
    "meta-llama/llama-3.3-70b-instruct",
    
    # DeepSeek
    "deepseek/deepseek-chat",
    "deepseek/deepseek-chat-v3-0324",
    "deepseek/deepseek-chat-v3-0324:free",
    "deepseek/deepseek-r1",
    "deepseek/deepseek-r1:free",
    "deepseek/deepseek-r1-distill-llama-70b",
    # Other
    "openrouter/quasar-alpha"
]

# [xai]
# models = [
#     "grok-1",
#     # Add other XAI models
# ]

# [phind]
# models = [
#     "Phind-70B",
#     # Add other Phind models
# ]
