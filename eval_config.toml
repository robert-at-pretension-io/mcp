
# Path to the main MCP Host configuration file.
# This is needed so the evaluator can potentially start/use tool servers defined there.
# Use tilde expansion for home directory.
mcp_host_config = "~/.config/mcp/mcp_host_config.json"

# Directory containing the evaluation task files (e.g., task_research_and_summarize.txt).
tasks_dir = "eval_tasks"

# Path to the prompt template used for grading responses.
# You need to create this file (see step 2 below).
grading_prompt_path = "prompts/eval_grading_prompt.txt"

# Path where the evaluation results (JSON Lines format) will be saved.
output_path = "eval_results/results.jsonl"

# Maximum time (in seconds) allowed for a model to complete a single task.
task_timeout_secs = 180 # 10 minutes

# Maximum time (in seconds) allowed for a model to grade a single response.
grading_timeout_secs = 180 # 3 minutes

# List of providers and models to evaluate.
# Each entry will act as both a "performer" (running the task)
# and a "grader" (evaluating other models' responses).
# API keys are primarily expected via environment variables (e.g., OPENROUTER_API_KEY, GROQ_API_KEY)
# or a .env file, as set up in main.rs/main_repl.rs.
# You can optionally add `api_key = "..."` to override for a specific entry.
[[providers]]
name = "openrouter"
model = "openai/gpt-4o"


[[providers]]
name = "openrouter"
model = "openai/o3-mini"

[[providers]]
name = "openrouter"
model = "anthropic/claude-3.5-sonnet"

[[providers]]
name = "openrouter"
model = "anthropic/claude-3.7-sonnet"

[[providers]]
name = "openrouter"
model = "google/gemini-2.5-pro-exp-03-25"

[[providers]]
name = "openrouter"
model = "meta-llama/llama-3.1-70b-instruct"

[[providers]]
name = "openrouter"
model = "mistralai/mistral-large"

[[providers]]
name = "openrouter"
model = "deepseek/deepseek-chat" # Or deepseek-v3-0324

[[providers]]
name = "groq"
model = "llama3-70b-8192" # Direct Groq for speed

[[providers]]
name = "openrouter"
model = "openai/gpt-4o-mini" # Include a smaller OpenAI model
